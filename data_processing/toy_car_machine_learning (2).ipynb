{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toy_car_machine_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import  pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.svm as sk\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#read the csv file\n",
        "featuretable= pd.read_csv('/content/all features with label.csv') \n",
        "print(featuretable)\n",
        "\n",
        "featuretable= np.array(featuretable)\n",
        "Ftotal_45=np.transpose(featuretable)\n",
        "Ftotal_45= np.delete(Ftotal_45, 0, 0) #remove label of features\n",
        "Ftotal_45= np.array(Ftotal_45,dtype=np.float)\n",
        "X= np.delete(Ftotal_45, 48, 1) #remove label from other features\n",
        "y= Ftotal_45[:, 48] #label of set\n",
        "\n",
        "#find features with high correlation to remove them\n",
        "df = pd.DataFrame(X)\n",
        "print('df.head()=') \n",
        "print(df.head())    \n",
        "# Create correlation matrix\n",
        "corr_matrix = df.corr().abs()\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "# Find features with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
        "# Drop features \n",
        "df.drop(to_drop, axis=1, inplace=True)\n",
        "print()\n",
        "print('to_drop=')\n",
        "print(to_drop)\n",
        "\n",
        "# create dataset\n",
        "Ftotal=np.transpose(Ftotal_45)\n",
        "Ftotal= np.delete(Ftotal,[41,42,43,44],1)\n",
        "Fselect= np.array([Ftotal[4,:],Ftotal[14,:],Ftotal[30,:],Ftotal[33,:],Ftotal[37,:],Ftotal[43,:],Ftotal[44,:],Ftotal[48,:]]) #choosing features+labels as input of SVM\n",
        "Fselect=np.transpose(Fselect)\n",
        "X= np.delete(Fselect, [7], 1) #remove label\n",
        "y= Fselect[:,7] #select label\n",
        "\n",
        "#print('true')\n",
        "#print(y)\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "\n",
        "# create model \n",
        "model = sk.SVC(kernel='linear') #linear, rbf, or poly\n",
        "#model = skn.MLPClassifier(activation='logistic') #identity, logistic, relu\n",
        "#model = skm.RandomForestClassifier() \n",
        "\n",
        "\n",
        "#sk.SVC.fit(X,y)\n",
        "#df= sk.SVC.decision_function(X)\n",
        "#print('predicted')\n",
        "#print(df)\n",
        "#plot_confusion_matrix(model, X, y)\n",
        "\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) #accuracy , recall, precision\n",
        "# report performance\n",
        "print('accuracy: %.5f (%.5f)' % (np.mean(scores), np.std(scores)))\n"
      ],
      "metadata": {
        "id": "B9yhOS6ZnAnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e888a2b-9849-4413-be53-99fb0f1c1fed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      features            95           100           106           112  \\\n",
            "0         fx_1     21.731400     11.074600      5.834750      3.777310   \n",
            "1         fx_2     22.215700     12.042800      6.920830      4.017150   \n",
            "2         fx_3     26.185700     20.578400      6.993340      4.045540   \n",
            "3         fx_4     27.611400     21.523000      7.797530      4.281230   \n",
            "4         fx_5     29.351900     44.027300     10.877900      4.476160   \n",
            "5         fy_1     15.503900     14.723200     15.694200     15.766700   \n",
            "6         fy_2     15.926400     15.124300     16.121800     16.196300   \n",
            "7         fy_3     17.451000     16.572200     17.665100     17.746800   \n",
            "8         fy_4     20.214900     19.196800     20.462900     20.557500   \n",
            "9         fy_5     29.281100     27.806500     29.640400     29.777400   \n",
            "10        fz_1     30.723200     30.403200      9.211280      9.955450   \n",
            "11        fz_2     41.451400     40.121700     10.535400     12.565200   \n",
            "12        fz_3     53.052900     51.192200     15.030200     13.477800   \n",
            "13        fz_4     68.561800     60.530300     15.159300     28.886000   \n",
            "14        fz_5     72.266900     96.948200     21.383700     31.715900   \n",
            "15        Ax_1      0.013131      0.013909      0.005624      0.128916   \n",
            "16        Ax_2      0.007576      0.017266      0.004090      0.004109   \n",
            "17        Ax_3      0.009091      0.004796      0.013292      0.040062   \n",
            "18        Ax_4      0.004545      0.007674      0.008691      0.002054   \n",
            "19        Ax_5      0.003535      0.003357      0.010736      0.007191   \n",
            "20        Ay_1      0.004545      0.004317      0.004601      0.004622   \n",
            "21        Ay_2      0.008081      0.007674      0.008180      0.008218   \n",
            "22        Ay_3      0.005556      0.005276      0.005624      0.005650   \n",
            "23        Ay_4      0.003030      0.002878      0.003067      0.003082   \n",
            "24        Ay_5      0.001515      0.001439      0.001534      0.001541   \n",
            "25        Az_1      0.006566      0.006235      0.025051      0.016949   \n",
            "26        Az_2      0.005556      0.007194      0.014826      0.020031   \n",
            "27        Az_3      0.004040      0.004796      0.021984      0.009245   \n",
            "28        Az_4      0.003030      0.002878      0.019427      0.006163   \n",
            "29        Az_5      0.002020      0.001918      0.012270      0.004109   \n",
            "30          Ex  14103.500000  19740.200000   2128.150000   1809.790000   \n",
            "31          Ey  12598.300000  26197.400000   3619.480000  12151.200000   \n",
            "32          Ez  25357.300000  55093.400000  11377.100000  14218.500000   \n",
            "33        Coxy      0.213901      0.048058      0.250448     -0.264854   \n",
            "34        Coxz     -0.102032      0.314899      0.498143     -0.141149   \n",
            "35        Coyz     -0.543378     -0.077862      0.152940      0.291894   \n",
            "36          Jx      0.266234      0.241255      0.177026      0.157890   \n",
            "37      num_Jx      4.000000      6.000000      1.000000      2.000000   \n",
            "38          Jy      0.296711      0.248364      0.174382      0.263674   \n",
            "39      num_Jy      6.000000      7.000000      1.000000      2.000000   \n",
            "40        time    178.015000    180.023000    179.738000    179.940000   \n",
            "41       turns    490.000000    457.000000    488.000000    100.000000   \n",
            "42  diff turns    152.000000    180.000000    317.000000     66.000000   \n",
            "43  diff /time      0.853861      0.999872      1.763680      0.366789   \n",
            "44     wheel r      0.086364      0.020115      0.395706      0.541538   \n",
            "45  not play r      0.219697      0.000000      0.052147      0.026154   \n",
            "46      play r      0.651515      0.902299      0.495399      0.372308   \n",
            "47     other r      0.042424      0.077586      0.056749      0.060000   \n",
            "48         NaN      1.000000      1.000000      1.000000      1.000000   \n",
            "\n",
            "             118          201           202          203           204  ...  \\\n",
            "0      11.665600     7.339540      9.203240     9.595330     32.913900  ...   \n",
            "1      11.889900     8.934360     10.509700    10.257900     41.126500  ...   \n",
            "2      14.852600     9.333040     10.900400    11.499500     41.527700  ...   \n",
            "3      15.820800    10.109600     11.216700    12.192900     51.246000  ...   \n",
            "4      18.166000    15.113900     11.230700    12.264800     60.233800  ...   \n",
            "5      16.165200     6.334390     17.431900     4.156520      6.700880  ...   \n",
            "6      16.605700     7.272840     17.912400     5.382400      7.083140  ...   \n",
            "7      18.195300     7.468890     18.017700     5.397190      7.596790  ...   \n",
            "8      21.077100    12.281100     19.649500    10.746800      9.119770  ...   \n",
            "9      30.530100    22.145100     24.018200    23.331600      9.322370  ...   \n",
            "10     28.337100     6.139380     34.793100    19.085500     11.683500  ...   \n",
            "11     30.120600     6.413660     36.060200    24.152000     13.742500  ...   \n",
            "12     32.552100     7.682870     37.288600    25.489300     15.948600  ...   \n",
            "13     38.535600     9.625820     38.556400    26.043200     17.652100  ...   \n",
            "14     90.017400    10.594400     49.091200    29.389300     23.937800  ...   \n",
            "15      0.020537     0.001993      0.000773     0.001479      0.000331  ...   \n",
            "16      0.011585     0.001514      0.000414     0.001683      0.000265  ...   \n",
            "17      0.009479     0.001196      0.001160     0.001224      0.000199  ...   \n",
            "18      0.005266     0.001036      0.000138     0.000408      0.000110  ...   \n",
            "19      0.002633     0.000877      0.000248     0.000663      0.000155  ...   \n",
            "20      0.004739     0.002471      0.000138     0.001224      0.000088  ...   \n",
            "21      0.008425     0.001514      0.000635     0.001581      0.000486  ...   \n",
            "22      0.005793     0.002072      0.000387     0.001377      0.000420  ...   \n",
            "23      0.003160     0.000877      0.000331     0.000714      0.000265  ...   \n",
            "24      0.001580     0.000558      0.000193     0.000255      0.000221  ...   \n",
            "25      0.014745     0.002152      0.000828     0.000663      0.000309  ...   \n",
            "26      0.008425     0.001993      0.000442     0.000561      0.000398  ...   \n",
            "27      0.004739     0.001514      0.000607     0.000459      0.000221  ...   \n",
            "28      0.007372     0.001036      0.000138     0.000306      0.000508  ...   \n",
            "29      0.002633     0.000319      0.000083     0.000204      0.000133  ...   \n",
            "30   5501.900000  2567.880000   4244.460000  2487.270000  14736.200000  ...   \n",
            "31  14313.900000  3406.480000   9322.550000  3803.230000   1128.640000  ...   \n",
            "32  33171.600000  2424.700000  25456.600000  6357.190000   4966.990000  ...   \n",
            "33      0.080166     0.418712      0.066910    -0.093966     -0.410278  ...   \n",
            "34      0.181687     0.496579      0.201201     0.232787     -0.524316  ...   \n",
            "35      0.348190     0.757259      0.487594     0.827563      0.157732  ...   \n",
            "36      0.203646     0.183601      0.163008     0.150323      0.123384  ...   \n",
            "37      3.000000     3.000000      6.000000     3.000000      3.000000  ...   \n",
            "38      0.298397     0.177905      0.254822     0.189413      0.129485  ...   \n",
            "39      4.000000     2.000000      8.000000     1.000000      4.000000  ...   \n",
            "40    179.875000   139.400000    362.200000   217.900000    452.500000  ...   \n",
            "41    378.000000   123.000000    580.000000   249.000000     23.000000  ...   \n",
            "42    230.000000    96.000000    456.000000   187.000000     17.000000  ...   \n",
            "43      1.278670     0.688666      1.258970     0.858192      0.037569  ...   \n",
            "44      0.042587     0.000000      0.000000     0.000000      0.000000  ...   \n",
            "45      0.116719     0.000000      0.000000     0.000000      0.000000  ...   \n",
            "46      0.763407     0.983512      0.971294     0.975688      0.992045  ...   \n",
            "47      0.077287     0.016487      0.028705     0.024312      0.007954  ...   \n",
            "48      1.000000     1.000000      1.000000     1.000000      1.000000  ...   \n",
            "\n",
            "           310         311          312          313          314  \\\n",
            "0     0.996205    2.252410     3.055740     3.607510     1.466850   \n",
            "1     1.185990    2.297560     3.063410     3.709480     1.529160   \n",
            "2     1.790290    2.428220     3.228330     3.796080     1.538380   \n",
            "3     1.953410    2.673800     3.483930     4.028390     1.574120   \n",
            "4     2.352150    2.681860     4.449130     4.059890     1.597960   \n",
            "5     2.292200    2.387040     4.130290     2.629790     7.156920   \n",
            "6     2.292790    2.518700     4.392830     2.652350     7.174060   \n",
            "7     2.411020    2.758790     4.514370     2.692350     7.369850   \n",
            "8     2.514000    2.797700     4.572750     2.775320     7.775070   \n",
            "9     2.621050    2.878580     4.628630     2.906290     7.901160   \n",
            "10    0.608673    1.557520     2.852250     2.091920     1.973400   \n",
            "11    0.613252    1.608260     2.915300     2.100220     2.019680   \n",
            "12    0.615272    1.624710     2.918870     2.215560     2.066770   \n",
            "13    0.620443    1.638670     2.934180     2.343110     2.216720   \n",
            "14    0.663566    1.696970     3.021530     2.366050     2.242460   \n",
            "15    0.002593    0.001867     0.001468     0.001742     0.001744   \n",
            "16    0.000519    0.000283     0.001957     0.000205     0.003571   \n",
            "17    0.000332    0.006110     0.001590     0.000307     0.000249   \n",
            "18    0.000104    0.001697     0.000550     0.002049     0.003073   \n",
            "19    0.000145    0.000735     0.000367     0.000137     0.000415   \n",
            "20    0.007737    0.003168     0.046911     0.030100     0.000747   \n",
            "21    0.008608    0.007807     0.047217     0.037572     0.006228   \n",
            "22    0.009396    0.002546     0.046239     0.038768     0.006726   \n",
            "23    0.006575    0.002263     0.049052     0.030502     0.006976   \n",
            "24    0.008566    0.003960     0.047645     0.000137     0.000166   \n",
            "25    0.032273    0.054650     0.047523     0.004440     0.012705   \n",
            "26    0.013689    0.019235     0.000183     0.005158     0.000249   \n",
            "27    0.005828    0.049219     0.048135     0.003894     0.000830   \n",
            "28    0.003816    0.039206     0.020306     0.005772     0.003654   \n",
            "29    0.052641    0.032473     0.038838     0.011169     0.016359   \n",
            "30  292.613000  592.574000   924.818000   756.562000   313.931000   \n",
            "31  721.314000  564.807000  2991.900000  1660.340000  3188.740000   \n",
            "32  169.171000  459.821000  1515.350000   743.829000  1005.230000   \n",
            "33   -0.002445   -0.018298     0.011234    -0.028070    -0.048837   \n",
            "34    0.119440   -0.048562     0.258779     0.169341    -0.068549   \n",
            "35    0.080829    0.038420     0.296325     0.179861    -0.279384   \n",
            "36    0.097480    0.114884     0.187504     0.181556     0.127189   \n",
            "37    3.000000    2.000000     7.000000     8.000000     1.000000   \n",
            "38    0.132740    0.196322     0.177636     0.131204     0.188877   \n",
            "39    5.000000    6.000000     3.000000     4.000000     4.000000   \n",
            "40  535.700000  196.400000   163.500000   325.300000   133.800000   \n",
            "41  934.000000  273.000000   136.000000  1788.000000   198.000000   \n",
            "42  602.000000  188.000000    67.000000   774.000000   130.000000   \n",
            "43    1.123760    0.957230     0.409786     2.379340     0.971599   \n",
            "44    0.322322    0.377800     0.404034     0.619238     0.210762   \n",
            "45    0.262224    0.225560     0.281784     0.021204     0.177130   \n",
            "46    0.329600    0.320773     0.269559     0.310079     0.485799   \n",
            "47    0.085852    0.075865     0.044621     0.049477     0.126308   \n",
            "48    0.000000    0.000000     0.000000     0.000000     0.000000   \n",
            "\n",
            "            315          108           120           121           124  \n",
            "0      3.702020     5.965390     31.132100     12.785800     13.374900  \n",
            "1      3.717370     6.011520     35.006900     15.908400     13.661900  \n",
            "2      3.891810     7.249590     43.236700     16.219300     15.060500  \n",
            "3      4.379110    11.548700     45.883900     21.862900     22.210100  \n",
            "4      4.735380    12.235900     61.819400     38.863600     39.578500  \n",
            "5      3.900460    15.718300     15.815500     16.165200     14.938100  \n",
            "6      4.152440    16.146600     16.246400     16.605700     15.345100  \n",
            "7      4.222540    17.692300     17.801600     18.195300     16.814100  \n",
            "8      4.397280    20.494300     20.621000     21.077100     19.477100  \n",
            "9      4.610240    29.685900     29.869400     30.530100     28.212500  \n",
            "10     4.240150     3.340740     29.884400     17.962300     18.283100  \n",
            "11     4.257110     3.967150     31.300300     22.864400     26.874400  \n",
            "12     4.370040     4.773230     39.655100     24.854700     29.465200  \n",
            "13     4.387590     7.558730     42.100700     29.439500     29.649800  \n",
            "14     4.876040    11.436100     53.067300     35.601000    100.522000  \n",
            "15     0.001168     0.051203      0.007213      0.009479      0.006813  \n",
            "16     0.001388     0.038914      0.008243      0.015798      0.004380  \n",
            "17     0.000548     0.023041      0.002576      0.007899      0.003406  \n",
            "18     0.000329     0.009729      0.001546      0.004739      0.005839  \n",
            "19     0.000767     0.013313      0.003606      0.001580      0.002433  \n",
            "20     0.014934     0.004610      0.004637      0.004740      0.004380  \n",
            "21     0.005620     0.008193      0.008243      0.008425      0.007786  \n",
            "22     0.002848     0.005632      0.005667      0.005793      0.005353  \n",
            "23     0.003651     0.003072      0.003091      0.003160      0.002920  \n",
            "24     0.003286     0.001536      0.001546      0.001580      0.001460  \n",
            "25     0.001789     0.058884      0.006700      0.008952      0.008273  \n",
            "26     0.005440     0.026626      0.009274      0.006846      0.004866  \n",
            "27     0.000949     0.020993      0.011850      0.007899      0.003406  \n",
            "28     0.000183     0.016385      0.004637      0.005793      0.005839  \n",
            "29     0.005952     0.010753      0.005667      0.004739      0.002433  \n",
            "30  1620.000000  1590.470000  27781.800000  14914.100000   8730.000000  \n",
            "31  2569.920000  1583.090000  19464.500000  16845.800000  10493.800000  \n",
            "32  3621.460000  1051.130000  43877.800000  37073.000000  42431.900000  \n",
            "33    -0.151186     0.231816     -0.046294     -0.051790     -0.395482  \n",
            "34    -0.218901     0.188707      0.350507      0.664218      0.540202  \n",
            "35     0.153981     0.399459      0.042394      0.252505      0.324947  \n",
            "36     0.133673     0.226100      0.258630      0.267740      0.290781  \n",
            "37     4.000000     2.000000      6.000000      3.000000      4.000000  \n",
            "38     0.166507     0.256064      0.264268      0.329446      0.349764  \n",
            "39     6.000000     3.000000      8.000000      4.000000      4.000000  \n",
            "40   304.300000   179.991000    180.287000    179.969000    190.306000  \n",
            "41  1301.000000   451.000000    150.000000     69.000000     62.000000  \n",
            "42   578.000000   311.000000     88.000000     38.000000     41.000000  \n",
            "43     1.899440     1.727860      0.488111      0.211147      0.215442  \n",
            "44     0.337714     0.177914      0.058642      0.000000      0.000000  \n",
            "45     0.208935     0.101227      0.007716      0.345426      0.188047  \n",
            "46     0.378120     0.679448      0.890432      0.610410      0.734694  \n",
            "47     0.075229     0.041411      0.043210      0.044164      0.077259  \n",
            "48     0.000000     0.000000      0.000000      0.000000      0.000000  \n",
            "\n",
            "[49 rows x 46 columns]\n",
            "df.head()=\n",
            "         0         1         2         3         4        5        6   \\\n",
            "0  21.73140  22.21570  26.18570  27.61140  29.35190  15.5039  15.9264   \n",
            "1  11.07460  12.04280  20.57840  21.52300  44.02730  14.7232  15.1243   \n",
            "2   5.83475   6.92083   6.99334   7.79753  10.87790  15.6942  16.1218   \n",
            "3   3.77731   4.01715   4.04554   4.28123   4.47616  15.7667  16.1963   \n",
            "4  11.66560  11.88990  14.85260  15.82080  18.16600  16.1652  16.6057   \n",
            "\n",
            "        7        8        9   ...        38   39       40     41     42  \\\n",
            "0  17.4510  20.2149  29.2811  ...  0.296711  6.0  178.015  490.0  152.0   \n",
            "1  16.5722  19.1968  27.8065  ...  0.248364  7.0  180.023  457.0  180.0   \n",
            "2  17.6651  20.4629  29.6404  ...  0.174382  1.0  179.738  488.0  317.0   \n",
            "3  17.7468  20.5575  29.7774  ...  0.263674  2.0  179.940  100.0   66.0   \n",
            "4  18.1953  21.0771  30.5301  ...  0.298397  4.0  179.875  378.0  230.0   \n",
            "\n",
            "         43        44        45        46        47  \n",
            "0  0.853861  0.086364  0.219697  0.651515  0.042424  \n",
            "1  0.999872  0.020115  0.000000  0.902299  0.077586  \n",
            "2  1.763680  0.395706  0.052147  0.495399  0.056749  \n",
            "3  0.366789  0.541538  0.026154  0.372308  0.060000  \n",
            "4  1.278670  0.042587  0.116719  0.763407  0.077287  \n",
            "\n",
            "[5 rows x 48 columns]\n",
            "\n",
            "to_drop=\n",
            "[1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 17, 21, 22, 23, 32, 42, 43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.85556 (0.08810)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "from typing import Tuple\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def cross_val_predict(model, kfold :KFold, X : np.array, y : np.array) -> Tuple[np.array, np.array, np.array]:\n",
        "\n",
        "    model_ = cp.deepcopy(model)\n",
        "    \n",
        "    no_classes = len(np.unique(y))\n",
        "    \n",
        "    actual_classes = np.empty([0], dtype=int)\n",
        "    predicted_classes = np.empty([0], dtype=int)\n",
        "    predicted_proba = np.empty([0, no_classes]) \n",
        "\n",
        "    for train_ndx, test_ndx in kfold.split(X):\n",
        "\n",
        "        train_X, train_y, test_X, test_y = X[train_ndx], y[train_ndx], X[test_ndx], y[test_ndx]\n",
        "\n",
        "        actual_classes = np.append(actual_classes, test_y)\n",
        "\n",
        "        model_.fit(train_X, train_y)\n",
        "        predicted_classes = np.append(predicted_classes, model_.predict(test_X))\n",
        "\n",
        "        try:\n",
        "            predicted_proba = np.append(predicted_proba, model_.predict_proba(test_X), axis=0)\n",
        "        except:\n",
        "            predicted_proba = np.append(predicted_proba, np.zeros((len(test_X), no_classes), dtype=float), axis=0)\n",
        "\n",
        "    return actual_classes, predicted_classes, predicted_proba\n",
        "\n",
        "def plot_confusion_matrix(actual_classes : np.array, predicted_classes : np.array):\n",
        "\n",
        "    matrix = confusion_matrix(actual_classes, predicted_classes)\n",
        "    \n",
        "    plt.figure(figsize=(12.8,6))\n",
        "    sns.heatmap(matrix, annot=True, xticklabels=\"auto\" , yticklabels=\"auto\", cmap=\"Blues\", fmt=\"g\")\n",
        "    plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4x7s2j4-QV1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.svm as sk\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#read the csv file\n",
        "featuretable= pd.read_csv('/content/all features with label.csv') \n",
        "print(featuretable)\n",
        "\n",
        "featuretable= np.array(featuretable)\n",
        "Ftotal_45=np.transpose(featuretable)\n",
        "Ftotal_45= np.delete(Ftotal_45, 0, 0) #remove label of features\n",
        "Ftotal_45= np.array(Ftotal_45,dtype=np.float)\n",
        "X= np.delete(Ftotal_45, 48, 1) #remove label from other features\n",
        "y= Ftotal_45[:, 48] #label of set\n",
        "\n",
        "# create dataset\n",
        "Ftotal=np.transpose(Ftotal_45)\n",
        "Ftotal= np.delete(Ftotal,[41,42,43,44],1)\n",
        "Fselect= np.array([Ftotal[4,:],Ftotal[14,:],Ftotal[30,:],Ftotal[33,:],Ftotal[37,:],Ftotal[43,:],Ftotal[44,:],Ftotal[48,:]]) #choosing features+labels as input of SVM\n",
        "Fselect=np.transpose(Fselect)\n",
        "X= np.delete(Fselect, [7], 1) #remove label\n",
        "y= Fselect[:,7] #select label\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "\n",
        "# create model \n",
        "model = sk.SVC(kernel='linear') #linear, rbf, or poly\n",
        "\n",
        "actual_classes, predicted_classes, _ = cross_val_predict(model, cv, X, y)\n",
        "print('actual_classes')\n",
        "print(actual_classes)\n",
        "print('predicted_classes')\n",
        "print(predicted_classes)\n",
        "plot_confusion_matrix(actual_classes, predicted_classes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H52fdbptRsbL",
        "outputId": "402d5a6b-7417-4212-f91f-665d13b135dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      features            95           100           106           112  \\\n",
            "0         fx_1     21.731400     11.074600      5.834750      3.777310   \n",
            "1         fx_2     22.215700     12.042800      6.920830      4.017150   \n",
            "2         fx_3     26.185700     20.578400      6.993340      4.045540   \n",
            "3         fx_4     27.611400     21.523000      7.797530      4.281230   \n",
            "4         fx_5     29.351900     44.027300     10.877900      4.476160   \n",
            "5         fy_1     15.503900     14.723200     15.694200     15.766700   \n",
            "6         fy_2     15.926400     15.124300     16.121800     16.196300   \n",
            "7         fy_3     17.451000     16.572200     17.665100     17.746800   \n",
            "8         fy_4     20.214900     19.196800     20.462900     20.557500   \n",
            "9         fy_5     29.281100     27.806500     29.640400     29.777400   \n",
            "10        fz_1     30.723200     30.403200      9.211280      9.955450   \n",
            "11        fz_2     41.451400     40.121700     10.535400     12.565200   \n",
            "12        fz_3     53.052900     51.192200     15.030200     13.477800   \n",
            "13        fz_4     68.561800     60.530300     15.159300     28.886000   \n",
            "14        fz_5     72.266900     96.948200     21.383700     31.715900   \n",
            "15        Ax_1      0.013131      0.013909      0.005624      0.128916   \n",
            "16        Ax_2      0.007576      0.017266      0.004090      0.004109   \n",
            "17        Ax_3      0.009091      0.004796      0.013292      0.040062   \n",
            "18        Ax_4      0.004545      0.007674      0.008691      0.002054   \n",
            "19        Ax_5      0.003535      0.003357      0.010736      0.007191   \n",
            "20        Ay_1      0.004545      0.004317      0.004601      0.004622   \n",
            "21        Ay_2      0.008081      0.007674      0.008180      0.008218   \n",
            "22        Ay_3      0.005556      0.005276      0.005624      0.005650   \n",
            "23        Ay_4      0.003030      0.002878      0.003067      0.003082   \n",
            "24        Ay_5      0.001515      0.001439      0.001534      0.001541   \n",
            "25        Az_1      0.006566      0.006235      0.025051      0.016949   \n",
            "26        Az_2      0.005556      0.007194      0.014826      0.020031   \n",
            "27        Az_3      0.004040      0.004796      0.021984      0.009245   \n",
            "28        Az_4      0.003030      0.002878      0.019427      0.006163   \n",
            "29        Az_5      0.002020      0.001918      0.012270      0.004109   \n",
            "30          Ex  14103.500000  19740.200000   2128.150000   1809.790000   \n",
            "31          Ey  12598.300000  26197.400000   3619.480000  12151.200000   \n",
            "32          Ez  25357.300000  55093.400000  11377.100000  14218.500000   \n",
            "33        Coxy      0.213901      0.048058      0.250448     -0.264854   \n",
            "34        Coxz     -0.102032      0.314899      0.498143     -0.141149   \n",
            "35        Coyz     -0.543378     -0.077862      0.152940      0.291894   \n",
            "36          Jx      0.266234      0.241255      0.177026      0.157890   \n",
            "37      num_Jx      4.000000      6.000000      1.000000      2.000000   \n",
            "38          Jy      0.296711      0.248364      0.174382      0.263674   \n",
            "39      num_Jy      6.000000      7.000000      1.000000      2.000000   \n",
            "40        time    178.015000    180.023000    179.738000    179.940000   \n",
            "41       turns    490.000000    457.000000    488.000000    100.000000   \n",
            "42  diff turns    152.000000    180.000000    317.000000     66.000000   \n",
            "43  diff /time      0.853861      0.999872      1.763680      0.366789   \n",
            "44     wheel r      0.086364      0.020115      0.395706      0.541538   \n",
            "45  not play r      0.219697      0.000000      0.052147      0.026154   \n",
            "46      play r      0.651515      0.902299      0.495399      0.372308   \n",
            "47     other r      0.042424      0.077586      0.056749      0.060000   \n",
            "48         NaN      1.000000      1.000000      1.000000      1.000000   \n",
            "\n",
            "             118          201           202          203           204  ...  \\\n",
            "0      11.665600     7.339540      9.203240     9.595330     32.913900  ...   \n",
            "1      11.889900     8.934360     10.509700    10.257900     41.126500  ...   \n",
            "2      14.852600     9.333040     10.900400    11.499500     41.527700  ...   \n",
            "3      15.820800    10.109600     11.216700    12.192900     51.246000  ...   \n",
            "4      18.166000    15.113900     11.230700    12.264800     60.233800  ...   \n",
            "5      16.165200     6.334390     17.431900     4.156520      6.700880  ...   \n",
            "6      16.605700     7.272840     17.912400     5.382400      7.083140  ...   \n",
            "7      18.195300     7.468890     18.017700     5.397190      7.596790  ...   \n",
            "8      21.077100    12.281100     19.649500    10.746800      9.119770  ...   \n",
            "9      30.530100    22.145100     24.018200    23.331600      9.322370  ...   \n",
            "10     28.337100     6.139380     34.793100    19.085500     11.683500  ...   \n",
            "11     30.120600     6.413660     36.060200    24.152000     13.742500  ...   \n",
            "12     32.552100     7.682870     37.288600    25.489300     15.948600  ...   \n",
            "13     38.535600     9.625820     38.556400    26.043200     17.652100  ...   \n",
            "14     90.017400    10.594400     49.091200    29.389300     23.937800  ...   \n",
            "15      0.020537     0.001993      0.000773     0.001479      0.000331  ...   \n",
            "16      0.011585     0.001514      0.000414     0.001683      0.000265  ...   \n",
            "17      0.009479     0.001196      0.001160     0.001224      0.000199  ...   \n",
            "18      0.005266     0.001036      0.000138     0.000408      0.000110  ...   \n",
            "19      0.002633     0.000877      0.000248     0.000663      0.000155  ...   \n",
            "20      0.004739     0.002471      0.000138     0.001224      0.000088  ...   \n",
            "21      0.008425     0.001514      0.000635     0.001581      0.000486  ...   \n",
            "22      0.005793     0.002072      0.000387     0.001377      0.000420  ...   \n",
            "23      0.003160     0.000877      0.000331     0.000714      0.000265  ...   \n",
            "24      0.001580     0.000558      0.000193     0.000255      0.000221  ...   \n",
            "25      0.014745     0.002152      0.000828     0.000663      0.000309  ...   \n",
            "26      0.008425     0.001993      0.000442     0.000561      0.000398  ...   \n",
            "27      0.004739     0.001514      0.000607     0.000459      0.000221  ...   \n",
            "28      0.007372     0.001036      0.000138     0.000306      0.000508  ...   \n",
            "29      0.002633     0.000319      0.000083     0.000204      0.000133  ...   \n",
            "30   5501.900000  2567.880000   4244.460000  2487.270000  14736.200000  ...   \n",
            "31  14313.900000  3406.480000   9322.550000  3803.230000   1128.640000  ...   \n",
            "32  33171.600000  2424.700000  25456.600000  6357.190000   4966.990000  ...   \n",
            "33      0.080166     0.418712      0.066910    -0.093966     -0.410278  ...   \n",
            "34      0.181687     0.496579      0.201201     0.232787     -0.524316  ...   \n",
            "35      0.348190     0.757259      0.487594     0.827563      0.157732  ...   \n",
            "36      0.203646     0.183601      0.163008     0.150323      0.123384  ...   \n",
            "37      3.000000     3.000000      6.000000     3.000000      3.000000  ...   \n",
            "38      0.298397     0.177905      0.254822     0.189413      0.129485  ...   \n",
            "39      4.000000     2.000000      8.000000     1.000000      4.000000  ...   \n",
            "40    179.875000   139.400000    362.200000   217.900000    452.500000  ...   \n",
            "41    378.000000   123.000000    580.000000   249.000000     23.000000  ...   \n",
            "42    230.000000    96.000000    456.000000   187.000000     17.000000  ...   \n",
            "43      1.278670     0.688666      1.258970     0.858192      0.037569  ...   \n",
            "44      0.042587     0.000000      0.000000     0.000000      0.000000  ...   \n",
            "45      0.116719     0.000000      0.000000     0.000000      0.000000  ...   \n",
            "46      0.763407     0.983512      0.971294     0.975688      0.992045  ...   \n",
            "47      0.077287     0.016487      0.028705     0.024312      0.007954  ...   \n",
            "48      1.000000     1.000000      1.000000     1.000000      1.000000  ...   \n",
            "\n",
            "           310         311          312          313          314  \\\n",
            "0     0.996205    2.252410     3.055740     3.607510     1.466850   \n",
            "1     1.185990    2.297560     3.063410     3.709480     1.529160   \n",
            "2     1.790290    2.428220     3.228330     3.796080     1.538380   \n",
            "3     1.953410    2.673800     3.483930     4.028390     1.574120   \n",
            "4     2.352150    2.681860     4.449130     4.059890     1.597960   \n",
            "5     2.292200    2.387040     4.130290     2.629790     7.156920   \n",
            "6     2.292790    2.518700     4.392830     2.652350     7.174060   \n",
            "7     2.411020    2.758790     4.514370     2.692350     7.369850   \n",
            "8     2.514000    2.797700     4.572750     2.775320     7.775070   \n",
            "9     2.621050    2.878580     4.628630     2.906290     7.901160   \n",
            "10    0.608673    1.557520     2.852250     2.091920     1.973400   \n",
            "11    0.613252    1.608260     2.915300     2.100220     2.019680   \n",
            "12    0.615272    1.624710     2.918870     2.215560     2.066770   \n",
            "13    0.620443    1.638670     2.934180     2.343110     2.216720   \n",
            "14    0.663566    1.696970     3.021530     2.366050     2.242460   \n",
            "15    0.002593    0.001867     0.001468     0.001742     0.001744   \n",
            "16    0.000519    0.000283     0.001957     0.000205     0.003571   \n",
            "17    0.000332    0.006110     0.001590     0.000307     0.000249   \n",
            "18    0.000104    0.001697     0.000550     0.002049     0.003073   \n",
            "19    0.000145    0.000735     0.000367     0.000137     0.000415   \n",
            "20    0.007737    0.003168     0.046911     0.030100     0.000747   \n",
            "21    0.008608    0.007807     0.047217     0.037572     0.006228   \n",
            "22    0.009396    0.002546     0.046239     0.038768     0.006726   \n",
            "23    0.006575    0.002263     0.049052     0.030502     0.006976   \n",
            "24    0.008566    0.003960     0.047645     0.000137     0.000166   \n",
            "25    0.032273    0.054650     0.047523     0.004440     0.012705   \n",
            "26    0.013689    0.019235     0.000183     0.005158     0.000249   \n",
            "27    0.005828    0.049219     0.048135     0.003894     0.000830   \n",
            "28    0.003816    0.039206     0.020306     0.005772     0.003654   \n",
            "29    0.052641    0.032473     0.038838     0.011169     0.016359   \n",
            "30  292.613000  592.574000   924.818000   756.562000   313.931000   \n",
            "31  721.314000  564.807000  2991.900000  1660.340000  3188.740000   \n",
            "32  169.171000  459.821000  1515.350000   743.829000  1005.230000   \n",
            "33   -0.002445   -0.018298     0.011234    -0.028070    -0.048837   \n",
            "34    0.119440   -0.048562     0.258779     0.169341    -0.068549   \n",
            "35    0.080829    0.038420     0.296325     0.179861    -0.279384   \n",
            "36    0.097480    0.114884     0.187504     0.181556     0.127189   \n",
            "37    3.000000    2.000000     7.000000     8.000000     1.000000   \n",
            "38    0.132740    0.196322     0.177636     0.131204     0.188877   \n",
            "39    5.000000    6.000000     3.000000     4.000000     4.000000   \n",
            "40  535.700000  196.400000   163.500000   325.300000   133.800000   \n",
            "41  934.000000  273.000000   136.000000  1788.000000   198.000000   \n",
            "42  602.000000  188.000000    67.000000   774.000000   130.000000   \n",
            "43    1.123760    0.957230     0.409786     2.379340     0.971599   \n",
            "44    0.322322    0.377800     0.404034     0.619238     0.210762   \n",
            "45    0.262224    0.225560     0.281784     0.021204     0.177130   \n",
            "46    0.329600    0.320773     0.269559     0.310079     0.485799   \n",
            "47    0.085852    0.075865     0.044621     0.049477     0.126308   \n",
            "48    0.000000    0.000000     0.000000     0.000000     0.000000   \n",
            "\n",
            "            315          108           120           121           124  \n",
            "0      3.702020     5.965390     31.132100     12.785800     13.374900  \n",
            "1      3.717370     6.011520     35.006900     15.908400     13.661900  \n",
            "2      3.891810     7.249590     43.236700     16.219300     15.060500  \n",
            "3      4.379110    11.548700     45.883900     21.862900     22.210100  \n",
            "4      4.735380    12.235900     61.819400     38.863600     39.578500  \n",
            "5      3.900460    15.718300     15.815500     16.165200     14.938100  \n",
            "6      4.152440    16.146600     16.246400     16.605700     15.345100  \n",
            "7      4.222540    17.692300     17.801600     18.195300     16.814100  \n",
            "8      4.397280    20.494300     20.621000     21.077100     19.477100  \n",
            "9      4.610240    29.685900     29.869400     30.530100     28.212500  \n",
            "10     4.240150     3.340740     29.884400     17.962300     18.283100  \n",
            "11     4.257110     3.967150     31.300300     22.864400     26.874400  \n",
            "12     4.370040     4.773230     39.655100     24.854700     29.465200  \n",
            "13     4.387590     7.558730     42.100700     29.439500     29.649800  \n",
            "14     4.876040    11.436100     53.067300     35.601000    100.522000  \n",
            "15     0.001168     0.051203      0.007213      0.009479      0.006813  \n",
            "16     0.001388     0.038914      0.008243      0.015798      0.004380  \n",
            "17     0.000548     0.023041      0.002576      0.007899      0.003406  \n",
            "18     0.000329     0.009729      0.001546      0.004739      0.005839  \n",
            "19     0.000767     0.013313      0.003606      0.001580      0.002433  \n",
            "20     0.014934     0.004610      0.004637      0.004740      0.004380  \n",
            "21     0.005620     0.008193      0.008243      0.008425      0.007786  \n",
            "22     0.002848     0.005632      0.005667      0.005793      0.005353  \n",
            "23     0.003651     0.003072      0.003091      0.003160      0.002920  \n",
            "24     0.003286     0.001536      0.001546      0.001580      0.001460  \n",
            "25     0.001789     0.058884      0.006700      0.008952      0.008273  \n",
            "26     0.005440     0.026626      0.009274      0.006846      0.004866  \n",
            "27     0.000949     0.020993      0.011850      0.007899      0.003406  \n",
            "28     0.000183     0.016385      0.004637      0.005793      0.005839  \n",
            "29     0.005952     0.010753      0.005667      0.004739      0.002433  \n",
            "30  1620.000000  1590.470000  27781.800000  14914.100000   8730.000000  \n",
            "31  2569.920000  1583.090000  19464.500000  16845.800000  10493.800000  \n",
            "32  3621.460000  1051.130000  43877.800000  37073.000000  42431.900000  \n",
            "33    -0.151186     0.231816     -0.046294     -0.051790     -0.395482  \n",
            "34    -0.218901     0.188707      0.350507      0.664218      0.540202  \n",
            "35     0.153981     0.399459      0.042394      0.252505      0.324947  \n",
            "36     0.133673     0.226100      0.258630      0.267740      0.290781  \n",
            "37     4.000000     2.000000      6.000000      3.000000      4.000000  \n",
            "38     0.166507     0.256064      0.264268      0.329446      0.349764  \n",
            "39     6.000000     3.000000      8.000000      4.000000      4.000000  \n",
            "40   304.300000   179.991000    180.287000    179.969000    190.306000  \n",
            "41  1301.000000   451.000000    150.000000     69.000000     62.000000  \n",
            "42   578.000000   311.000000     88.000000     38.000000     41.000000  \n",
            "43     1.899440     1.727860      0.488111      0.211147      0.215442  \n",
            "44     0.337714     0.177914      0.058642      0.000000      0.000000  \n",
            "45     0.208935     0.101227      0.007716      0.345426      0.188047  \n",
            "46     0.378120     0.679448      0.890432      0.610410      0.734694  \n",
            "47     0.075229     0.041411      0.043210      0.044164      0.077259  \n",
            "48     0.000000     0.000000      0.000000      0.000000      0.000000  \n",
            "\n",
            "[49 rows x 46 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual_classes\n",
            "[1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            "predicted_classes\n",
            "[1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 921.6x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding pvalue\n",
        "\n",
        "import  pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.svm as sk\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import permutation_test_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#read the csv file\n",
        "featuretable= pd.read_csv('/content/all features with label.csv') \n",
        "print(featuretable)\n",
        "\n",
        "featuretable= np.array(featuretable)\n",
        "Ftotal_45=np.transpose(featuretable)\n",
        "Ftotal_45= np.delete(Ftotal_45, 0, 0) #remove label of features\n",
        "Ftotal_45= np.array(Ftotal_45,dtype=np.float)\n",
        "X= np.delete(Ftotal_45, 48, 1) #remove label from other features\n",
        "y= Ftotal_45[:, 48] #label of set\n",
        "\n",
        "# create dataset\n",
        "Ftotal=np.transpose(Ftotal_45)\n",
        "Ftotal= np.delete(Ftotal,[41,42,43,44],1)\n",
        "Fselect= np.array([Ftotal[4,:],Ftotal[14,:],Ftotal[30,:],Ftotal[33,:],Ftotal[37,:],Ftotal[43,:],Ftotal[44,:],Ftotal[48,:]]) #choosing features+labels as input of SVM\n",
        "Fselect=np.transpose(Fselect)\n",
        "X= np.delete(Fselect, [7], 1) #remove label\n",
        "y= Fselect[:,7] #select label\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "\n",
        "# create model \n",
        "model = sk.SVC(kernel='linear') #linear, rbf, or poly\n",
        "#model = skn.MLPClassifier(activation='logistic') #identity, logistic, relu\n",
        "#model = skm.RandomForestClassifier() \n",
        "\n",
        "#finding pvalue\n",
        "score, permutation_scores, pvalue = permutation_test_score(model, X, y, scoring='accuracy', cv=cv, n_permutations=10)\n",
        "print('pvalue: %.5f (%.5f)' % (np.mean(pvalue), np.std(pvalue)))\n",
        "\n",
        "#model.fit(X,y)\n",
        "#import random\n",
        "#score = model.score(X,y)\n",
        "#n_permutations = 1000\n",
        "#permutation_scores = np.zeros(n_permutations)\n",
        "#for i in range(n_permutations):\n",
        "#    y_permuted = random.shuffle(y)\n",
        "#    permutation_score = model.score(X, y_permuted)\n",
        "#    permutation_scores[i]=permutation_score\n",
        "\n",
        "#permutation_scores = np.array(permutation_scores)\n",
        "#pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)\n"
      ],
      "metadata": {
        "id": "OF71GfDT4ttB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}